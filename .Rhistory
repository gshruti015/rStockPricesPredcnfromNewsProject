summary(reg)
print("Predicted Fare")
print(predict.lm(reg, data.frame(COUPON =1, NEW = 3, VACATION = "No", SW = "No", HI = 6000, S_INCOME = 2000, E_INCOME = 2000, S_POP = 4000000, E_POP = 7150000, SLOT = "Free", GATE = "Constrained", DISTANCE = 1000, PAX = 6000)))
detach(data)
data<-read.csv("Airline Data V2.csv")
summary(data)
attach(data)
pairs((data[, sapply(data, is.numeric)]), pch = 25)
plot(DISTANCE, FARE, pch=16, col = SW)
legend('topleft', legend = levels(SW), pch = 16, col=unique(SW))
tmp<-round(cor(data[, sapply(data, is.numeric)], use = "complete.obs"),2)
tmp[tmp > abs(0.7)]<- NA
tmp
pairs((data[, sapply(data, is.numeric)]), pch = 25)
plot(DISTANCE, FARE, pch=16, col = SW)
legend('topleft', legend = levels(SW), pch = 16, col=unique(SW))
tmp<-round(cor(data[, sapply(data, is.numeric)], use = "complete.obs"),2)
tmp[tmp > abs(0.7)]<- NA
tmp
detach(data)
detach(data)
data<-read.csv("Airline Data V2.csv")
attach(data)
reg<-lm(FARE~COUPON+NEW+VACATION+SW+HI+S_INCOME+E_INCOME+S_POP+E_POP+SLOT+GATE+DISTANCE+PAX, data=data)
summary(reg)
print("Predicted Fare")
print(predict.lm(reg, data.frame(COUPON =1, NEW = 3, VACATION = "No", SW = "No", HI = 6000, S_INCOME = 2000, E_INCOME = 2000, S_POP = 4000000, E_POP = 7150000, SLOT = "Free", GATE = "Constrained", DISTANCE = 1000, PAX = 6000)))
detach(data)
detach(data)
data<-read.csv("Airline Data V2.csv")
attach(data)
reg<-lm(FARE~COUPON+NEW+VACATION+SW+HI+S_INCOME+E_INCOME+S_POP+E_POP+SLOT+GATE+DISTANCE+PAX, data=data)
summary(reg)
print("Predicted Fare")
print(predict.lm(reg, data.frame(COUPON =1, NEW = 3, VACATION = "No", SW = "No", HI = 6000, S_INCOME = 2000, E_INCOME = 2000, S_POP = 4000000, E_POP = 7150000, SLOT = "Free", GATE = "Constrained", DISTANCE = 1000, PAX = 6000)))
detach(data)
detach(data)
attach(data)
pairs((data[, sapply(data, is.numeric)]), pch = 25)
plot(DISTANCE, FARE, pch=16, col = SW)
legend('topleft', legend = levels(SW), pch = 16, col=unique(SW))
tmp<-round(cor(data[, sapply(data, is.numeric)], use = "complete.obs"),2)
tmp[tmp > abs(0.7)]<- NA
tmp
detach(data)
attach(data)
pairs((data[, sapply(data, is.numeric)]), pch = 25)
plot(DISTANCE, FARE, pch=16, col = SW)
legend('topleft', legend = levels(SW), pch = 16, col=unique(SW))
tmp<-round(cor(data[, sapply(data, is.numeric)], use = "complete.obs"),2)
tmp[tmp > abs(0.7)]<- NA
tmp
detach(data)
attach(data)
pairs((data[, sapply(data, is.numeric)]), pch = 25)
plot(DISTANCE, FARE, pch=16, col = SW)
legend('topleft', legend = levels(SW), pch = 16, col=unique(SW))
tmp<-round(cor(data[, sapply(data, is.numeric)], use = "complete.obs"),2)
tmp[tmp > abs(0.7)]<- NA
table(tmp)
detach(data)
attach(data)
pairs((data[, sapply(data, is.numeric)]), pch = 25)
plot(DISTANCE, FARE, pch=16, col = SW)
legend('topleft', legend = levels(SW), pch = 16, col=unique(SW))
tmp<-round(cor(data[, sapply(data, is.numeric)], use = "complete.obs"),2)
tmp[tmp > abs(0.7)]<- NA
tmp
detach(data)
summary(lm(FARE~SW, data=data))
coef(lm(FARE~SW, data=data))
coef(reg)
summary(lm(FARE~COUPON+NEW+VACATION*SW+HI+S_INCOME+E_INCOME+S_POP+E_POP+SLOT+GATE+DISTANCE+PAX, data=data))
summary(lm(FARE~SW, data=data))
coef(lm(FARE~SW, data=data))
coef(reg)
library(ggplot2)
library(ggplot2)
library(data.table)
theme_set(theme_economist_white())
df<-fread("Economist_Assignment_Data.csv",drop=1)
pointsToLabel <- c("Russia", "Venezuela", "Iraq", "Myanmar", "Sudan",
"Afghanistan", "Congo", "Greece", "Argentina", "Brazil",
"India", "Italy", "China", "South Africa", "Spane",
"Botswana", "Cape Verde", "Bhutan", "Rwanda", "France",
"United States", "Germany", "Britain", "Barbados", "Norway", "Japan",
"New Zealand", "Singapore")
cap<-function()
{
pl<-ggplot(df,aes(x=CPI,y=HDI))
pl2<-pl+geom_point(aes(color=factor(Region),size=3,shape=1)) + scale_shape_identity()
pl3<-pl2+ geom_smooth(aes(group=1),method='lm',formula = y~log(x),se=FALSE,color='blue')
pl4<-pl3 + geom_text(aes(label = Country), color = "gray20",
data = subset(df, Country %in% pointsToLabel),check_overlap = TRUE)
pl5<-pl4+scale_x_continuous(name = 'Corruption Perception Index, 2011 (10=least corrupt)',limits=c(1,10), breaks = 1:10)
pl6<-pl5+scale_y_continuous(name = 'Human Development Index, 2011 (1=Best)')
print(pl6+ggtitle('Corruption and Human Development')+ theme_economist_white())
}
cap()
source('~/Desktop/Utility-All/Udemy - R Bootcamp/R Bootcamp/Machine Learning with R/done/nlpPython.R')
#CONNECT TO TWITTER
setup_twitter_oauth('MbTpLWjPMkqxvLn1ckeY9oant','7vPHNOy625GlFIhD6qHBdxGtY4i4nl5Y80QBDvdBvPYPMSwnsF','2904866593-odP7vx2tbvqppw0hZmWTlMjRUOoZAwWjAbELsR3','EQ1m1C0Mhy3lfQxeG0Ce9AVP3uvpW61foPpGYR5CtztMX')
2
source('~/Desktop/Utility-All/Udemy - R Bootcamp/R Bootcamp/Machine Learning with R/done/nlpPython.R')
source('~/Desktop/Utility-All/Udemy - R Bootcamp/R Bootcamp/Machine Learning with R/done/nlpPython.R')
source('~/Desktop/Utility-All/Udemy - R Bootcamp/R Bootcamp/Machine Learning with R/done/nlpPython.R')
source('~/Desktop/Utility-All/Udemy - R Bootcamp/R Bootcamp/Machine Learning with R/done/nlpPython.R')
source('~/.active-rstudio-document')
source('~/Desktop/Utility-All/Udemy - R Bootcamp/R Bootcamp/Machine Learning with R/done/nlpPython.R')
source('~/.active-rstudio-document')
source('~/.active-rstudio-document')
source('~/.active-rstudio-document')
source('~/.active-rstudio-document')
source('~/.active-rstudio-document')
source('~/.active-rstudio-document')
source('~/.active-rstudio-document')
source('~/.active-rstudio-document')
source('~/.active-rstudio-document')
source('~/.active-rstudio-document')
source('~/.active-rstudio-document')
source('~/.active-rstudio-document')
source('~/.active-rstudio-document')
source('~/.active-rstudio-document')
source('~/.active-rstudio-document')
source('~/.active-rstudio-document')
source('~/.active-rstudio-document')
wordcloud(dm$word,dm$freq,random.order = FALSE,colors=brewer.pal(9,'Dark2'), size=0.1)
source('~/.active-rstudio-document')
source('~/.active-rstudio-document')
source('~/.active-rstudio-document')
source('~/.active-rstudio-document')
source('~/.active-rstudio-document')
source('~/Desktop/Budget2019_wordCloud.R')
source('~/Desktop/Budget2019_wordCloud.R')
source('~/Desktop/Budget2019_wordCloud.R')
source('~/Desktop/Budget2019_wordCloud.R')
source('~/Desktop/Budget2019_wordCloud.R')
source('~/Desktop/Budget2019_wordCloud.R')
source('~/Desktop/LinkedIn/nlp/Budget2019_wordCloud wthCredential.R')
source('~/Desktop/LinkedIn/nlp/Budget2019_wordCloud wthCredential.R')
?wordcloud
source('~/Desktop/LinkedIn/nlp/Budget2019_wordCloud wthCredential.R')
source('~/Desktop/LinkedIn/nlp/Budget2019_wordCloud wthCredential.R')
source('~/Desktop/LinkedIn/nlp/Budget2019_wordCloud wthCredential.R')
source('~/Desktop/LinkedIn/nlp/Budget2019_wordCloud wthCredential.R')
source('~/Desktop/LinkedIn/nlp/Budget2019_wordCloud wthCredential.R')
source('~/Desktop/LinkedIn/nlp/Budget2019_wordCloud wthCredential.R')
source('~/Desktop/LinkedIn/nlp/Budget2019_wordCloud wthCredential.R')
source('~/Desktop/LinkedIn/nlp/Budget2019_wordCloud wthCredential.R')
model<- glm(PREFERENCE ~ AGE+INCOME+GENDER, data = data.train, family = "binomial")
mydata<-read.csv("VoterPref.csv")
#mydata$PREFERENCE <- factor(mydata$Preference,levels=c("For","Against"))
mydata$PREFERENCE <- ifelse(mydata$PREFERENCE == "Against",1,0)
set.seed(71923)
datasplit <- sample(nrow(mydata),0.7*nrow(mydata))
data.train <- mydata[datasplit,]
data.test <- mydata[-datasplit,]
model<- glm(PREFERENCE ~ AGE+INCOME+GENDER, data = data.train, family = "binomial")
cutoff<-0.5
#predict the values on  training set
actual.train<-data.train$PREFERENCE
predict.train.prob<-predict(model,type='response')
predict.train<-ifelse(predict.train.prob > cutoff, 1, 0)
confusion1 <- table(actual.train, predict.train)
rownames(confusion1) <- c("For","Against")
colnames(confusion1) <- c("For","Against")
confusion1
#predict the values on test set
actual.test<-data.test$PREFERENCE
predict.test.prob<-predict(model,type='response',newdata = data.test )
predict.test<-ifelse(predict.test.prob > cutoff, 1, 0)
confusion2 <- table(actual.test, predict.test)
rownames(confusion2) <- c("For","Against")
colnames(confusion2) <- c("For","Against")
confusion2
#Computation of sensitivity, specificity, accuracy, error rate, PPV, NPV
TrainTP<- sum(predict.train == 1 & actual.train == 1)
TrainTN<- sum(predict.train == 0 & actual.train == 0)
TrainFP<- sum(predict.train == 1 & actual.train == 0)
TrainFN<- sum(predict.train == 0 & actual.train == 1)
cat("\nTrain Set")
accuracy.train <- (TrainTP + TrainTN)/nrow(data.train)
cat("\nAccuracy :",accuracy.train)
cat("\nError Rate :",1- accuracy.train)
sensitivity.train <- TrainTP/(TrainTP+TrainFN)
cat("\nSensitivity :",sensitivity.train)
specificity.train <- TrainTN/(TrainTN+TrainFP)
cat("\nSpecificity :",specificity.train)
PPV.train <- TrainTP/(TrainTP+TrainFP)
cat("\nPPV :",PPV.train)
NPV.train <- TrainTN/(TrainTN+TrainFN)
cat("\nNPV :",NPV.train )
TestTP<- sum(predict.test == 1 & actual.test == 1)
TestTN<- sum(predict.test== 0 & actual.test  == 0)
TestFP<- sum(predict.test == 1 & actual.test  == 0)
TestFN<- sum(predict.test== 0 & actual.test  == 1)
cat("\nTest Set")
accuracy.test <- (TestTP + TestTN)/nrow(data.test)
cat("\nAccuracy :",accuracy.test)
cat("\nError Rate :",1- accuracy.test)
sensitivity.test <- TestTP/(TestTP+TestFN)
cat("\nSensitivity :",sensitivity.test)
specificity.test <- TestTN/(TestTN+TestFP)
cat("\nSpecificity :",specificity.test)
PPV.test <- TestTP/(TestTP+TestFP)
cat("\nPPV :",PPV.test)
NPV.test <- TestTN/(TestTN+TestFN)
cat("\nNPV :",NPV.test )
#ROC for test Set
cutoffrange <- seq(0, 1, length = 100)
fpr <- numeric(100)
tpr <- numeric(100)
roc.table <- data.frame(Cutoff = cutoffrange, FPR = fpr,TPR = tpr)
for (i in 1:100) {
roc.table$FPR[i] <- sum(predict.test.prob > cutoffrange[i] & actual.test == 0)/sum(actual.test == 0)
roc.table$TPR[i] <- sum(predict.test.prob > cutoffrange[i] & actual.test == 1)/sum(actual.test == 1)
}
plot(TPR ~ FPR, data = roc.table, type = "o",xlab="1 - Specificity",ylab="Sensitivity",col="blue",lty=2)
abline(a = 0, b = 1, lty = 2,col="red")
#ROC curve for train Set
cutoffrange <- seq(0, 1, length = 100)
fpr <- numeric(100)
tpr <- numeric(100)
#
for (i in 1:100) {
roc.table$FPR[i] <- sum(predict.train.prob> cutoffrange[i] & actual.train == 0)/sum(actual.train == 0)
roc.table$TPR[i] <- sum(predict.train.prob > cutoffrange[i] & actual.train == 1)/sum(actual.train == 1)
}
lines(TPR~FPR,data = roc.table, type="o",col="green",lty=2)
cutoffrange <- seq(0, 1, length = 700)
accuracy.train <- numeric(700)
roc.table <- data.frame(Cutoff = cutoffrange, Accuracy = accuracy.train)
for (i in 1:700)
{
roc.table$Accuracy[i] <- (sum(predict.train.prob > cutoffrange[i] & actual.train == 1) + sum(predict.train.prob < cutoffrange[i] & actual.train == 0))/700
}
plot(Accuracy ~ Cutoff , data = roc.table, type = "o",xlab=" Cutoff",ylab="Accuracy",col="blue",lty=2)
var<-which.max(roc.table$Accuracy)
roc.table[var,]
cutoffrange <- seq(0, 1, length = 700)
accuracy.test <- numeric(700)
roc.table <- data.frame(Cutoff = cutoffrange, Accuracy = accuracy.test)
for (i in 1:700)
{
roc.table$Accuracy[i] <- (sum(predict.test.prob > cutoffrange[i] & actual.test == 1) + sum(predict.test.prob < cutoffrange[i] & actual.test == 0))/700
}
plot(Accuracy ~ Cutoff , data = roc.table, type = "o",xlab="Cutoff",ylab="Accuracy",col="blue",lty=2)
roc.table[var,]
q<-roc.table[var,]$Cutoff
q
actual.train<-data.train$PREFERENCE
predict.train.prob<-predict(model,type='response')
predict.train<-ifelse(predict.train.prob > q, 1, 0)
confusion3 <- table(actual.train, predict.train)
confusion3
# library(ROCR)
# #Training Set
# pred <- prediction(predict.train.prob, actual.train)
# perf <- performance( pred, "tpr", "fpr" )
# perf.acc <- performance( pred, "acc")
# plot( perf , show.spread.at=seq(0, 1, by=0.1), col="red")
#
# #Test Set
# pred <- prediction(predict.test.prob, actual.test)
# perf.tpr.fpr <- performance( pred, "tpr", "fpr" )
# perf.acc <- performance( pred, "acc")
# plot( perf , show.spread.at=seq(0, 1, by=0.1), col="red")
#training Data Lift Chart
library("data.table")
decile_Lift <- function(df)
{
df <- df[order(-df$predicted.probability),]
df$roworder <- 1:nrow(df)
baseline <- sum(df$actual.train) / 10
df$decile <- ceiling((df$roworder/nrow(df)) * 10)
dt <- data.table(df)
dt <- dt[, sum(actual.train), by = decile]
dt$baseline <- baseline
barplot(t(data.frame(dt$V1,dt$baseline)), main="Decile wise comparision of successes", xlab="Deciles",    col=c("darkblue","red"), beside=TRUE, names=dt$decile)
barplot(t(data.frame(dt$V1)/data.frame(dt$baseline)), main="Decile wise comparision of successes", xlab="Deciles", col=c("darkblue"), beside=TRUE, names=dt$decile)
}
actual.train<-data.train$PREFERENCE
predicted.probability<-predict(model,type='response')
df.train <- data.frame(predicted.probability,actual.train)
decile_Lift(df.train)
decile_Lift <- function(df)
{
df <- df[order(-df$predicted.probability),]
df$roworder <- 1:nrow(df)
baseline <- sum(df$actual.test) / 10
df$decile <- ceiling((df$roworder/nrow(df)) * 10)
dt <- data.table(df)
dt <- dt[, sum(actual.test), by = decile]
dt$baseline <- baseline
barplot(t(data.frame(dt$V1,dt$baseline)), main="Decile wise comparision of successes", xlab="Deciles",    col=c("darkblue","red"), beside=TRUE, names=dt$decile)
barplot(t(data.frame(dt$V1)/data.frame(dt$baseline)), main="Decile wise comparision of successes", xlab="Deciles", col=c("darkblue"), beside=TRUE, names=dt$decile)
}
actual.test<-data.test$PREFERENCE
predicted.probability<-predict(model,type='response', newdata = data.test)
df.test <- data.frame(predicted.probability,actual.test)
decile_Lift(df.test)
#training Data Lift Chart
actual.train<-data.train$PREFERENCE
predict.train.prob<-predict(model,type='response')
df1 <- data.frame(predict.train.prob,actual.train)
df1S <- df1[order(-predict.train.prob),]
df1S$Gains <- cumsum(df1S$actual.train)
plot(df1S$Gains,type="n",main="Training Data Gains Chart",xlab="Number of Cases",ylab="Cumulative Success")
lines(df1S$Gains)
abline(0,sum(df1S$actual.train)/nrow(df1S),lty = 2, col="red")
#
# Now the validation data gains chart
actual.test<-data.test$PREFERENCE
predict.test.prob<-predict(model,type='response', newdata = data.test)
df2 <- data.frame(predict.test.prob,actual.test)
df2S <- df2[order(-predict.test.prob),]
df2S$Gains <- cumsum(df2S$actual.test)
plot(df2S$Gains,type="n",main="Test Data Gains Chart",xlab="Number of Cases",ylab="Cumulative Success")
lines(df2S$Gains)
abline(0,sum(df2S$actual.test)/nrow(df1S),lty = 2, col="red")
setwd("~/Desktop/DataChallenge/National Cancer Institute")
source('~/.active-rstudio-document')
print(df)
df
print(head(df)
any(is.na(df)])
any(is.na(df)])
any(is.na(df))
df[,1]
source('~/.active-rstudio-document')
source('~/.active-rstudio-document')
any(is.na(df))
print(head(df)
install.packages("readxl")
source('~/.active-rstudio-document')
DF
df
source('~/.active-rstudio-document')
any(is.na(df))
source('~/.active-rstudio-document')
df
any(is.na(df2))
df2
ggplot(raw) + geom_bar(aes(x = Hair))
library(ggplot2)
source('~/.active-rstudio-document')
df
names(df)
which( colnames(df)=="CancerConfidentGetHealthInf" )
print(df[,30])
print(unique(df[,30]))
replace(df[,30], c(-9,-5), 0)
print(unique(df[,30]))
df[,30]<-replace(df[,30], c(-9,-5), 0)
print(unique(df[,30]))
df[,30]
df2 <- read_excel("NCI_HINTS_5_cycle2_public.xlsx")
source('~/.active-rstudio-document')
print(unique(df[,30]))
source('~/.active-rstudio-document')
source('~/.active-rstudio-document')
setwd("~/Downloads")
df$IntRsn_SharedSocNet[df$IntRsn_SharedSocNet == -9] <- NA
temp_col <- df$IntRsn_SharedSocNet
barplot(table(temp_col_no_9),main="Shared Health Information on Social Media",xlab=" Number of Patients", horiz=TRUE,
names.arg=c("yes", "NO"))
setwd("~/Desktop/DataChallenge/National Cancer Institute")
source('~/Downloads/plots.R')
df <- read.csv('NCI_HINTS_5_cycle2_public.xlsx', header = TRUE)
df
setwd("~/Downloads")
df <- read.csv('Cancer.csv', header = TRUE)
df
# Repalcing all mising values in last 12 months, have you used the internet to share health information on social networking sites
df$IntRsn_SharedSocNet[df$IntRsn_SharedSocNet == -9] <- NA
df$IntRsn_SharedSocNet == -9
temp_col <- df$IntRsn_SharedSocNet
barplot(table(temp_col_no_9),main="Shared Health Information on Social Media",xlab=" Number of Patients", horiz=TRUE,
names.arg=c("yes", "NO"))
temp_col <- df$IntRsn_SharedSocNet
temp_col_no_9 <- temp_col[temp_col != -9]
# load GGplot2
install.packages("ggplot2")
install.packages("ggplot2")
a <- table(temp_col_no_9)
a
# first plot
plot(temp_col_no_9)
barplot(table(temp_col_no_9),main="Shared Health Information on Social Media",xlab=" Number of Patients", horiz=TRUE,
names.arg=c("yes", "NO"))
barplot(table(temp_col_no_9),main="Shared Health Information on Social Media",ylab=" Number of Patients",
names.arg=c("yes", "NO"))
a
a$1
class(A)
class(a)
a[1,]
a[,1]
temp_col_no_9
a
a["1",]
a[,'1']
a[1,1]
b<-as.vector(a)
b
b[1]
# Third
p<-ggplot(data=df, aes(x=b[1], y=b[2])) + geom_bar(stat="identity")
p
# Third
p<-ggplot(data=df, aes(x=b[1], y=b[2])) + geom_bar()
p
# Third
p<-ggplot(data=table(temp_col_no_9), aes(x=b[1], y=b[2])) + geom_bar()
b<-as.data.frame(a)
b
b[1]
b[,2]
b$Freq[1]
# Third
p<-ggplot(data=b, aes(x=b$Freq[1], y=b$Freq[2])) + geom_bar()
p
# Third
p<-ggplot(data=b, aes(x=b$Freq[1], y=b$Freq[2])) + geom_bar(stat = 'bins')
p<-ggplot(data=b, aes(x=b$Freq[1], y=b$Freq[2])) + geom_bar(stat = 'bin')
p
# Third
p<-ggplot(data=b, aes(x=b$Freq[1]) + geom_histogram()
p
p
source('~/Downloads/plots.R')
# Third
p<-ggplot(data=b, aes(b$Freq[1]) + geom_histogram()
p
#install.packages("ggplot2")
#install.packages("Metrics")
# install.packages("e1071")
# install.packages("SparseM")
# install.packages("tm")
# install.packages("tree")
# install.packages("gbm")
# install.packages("forecast")
# install.packages("randomForest")
#install.packages("plotly")
#incluse.packages("wordcloud")
library('e1071')
library('SparseM')
library('tm')
library('Metrics')
library('ggplot2')
library('randomForest')
library('gbm')
library('plotly')
library('wordcloud')
#library("lattice")
#setwd("C:/Users/vjain1/Downloads/stocknews")
news <- read.csv("C:/Users/priya/OneDrive/Documents/Second sem materials/Data Mining and Predictive Analysis/Project/RedditNews.csv")
library('e1071')
library('SparseM')
library('tm')
library('Metrics')
library('ggplot2')
library('randomForest')
library('gbm')
library('plotly')
library("lattice")
library("corrplot")
library('GGally')
library('lubridate')
setwd("~/Desktop/Sem2/Data MIning - Kislay Prasad/Data Mining Project")
news <- read.csv("RedditNews.csv")
news$News <- as.character(news$News)
newsvector <- as.vector(news$News);    # Create vector
newssource <- VectorSource(newsvector); # Create source
newscorpus <- Corpus(newssource);       # Create corpus
newscorpus <- tm_map(newscorpus,stripWhitespace);
newscorpus <- tm_map(newscorpus,content_transformer(tolower));
newscorpus <- tm_map(newscorpus, removeWords,c("the","end",stopwords("english")));
newscorpus <- tm_map(newscorpus,removePunctuation);
newscorpus <- tm_map(newscorpus,removeNumbers);
inspect(newscorpus[1])
removeNumPunct <- function(x) gsub("[^[:alpha:][:space:]]*", "", x)
newscorpus <- tm_map(newscorpus, content_transformer(removeNumPunct))
tdm1 <- TermDocumentMatrix(newscorpus)
tdm1 = removeSparseTerms(tdm1, 0.99)
tdm1
# Create DocumentTermMattrix
dtm_matrix <- t(tdm1)
dtm_matrix
dtm_df <- data.frame(news$Date,as.matrix(dtm_matrix))
dtm_df
#Coombine news of same dates in one row - data had mulitple news rows for a single date
combined_news<-aggregate(dtm_df[-1], by=list(dtm_df$news.Date), sum)
df1<-read.csv("DJIA_table.csv")
combined_news$Date<-combined_news$Group.1
total <- merge(combined_news,df1,by="Date")
total$Group.1<-NULL
final<-total
final$change<-final$Close-final$Open
final$Date<-NULL
inTrain <- sample(nrow(final),0.7*nrow(final))
rf.Close=randomForest(change~.-Adj.Close-High-Low-Volume-Close-Open,data=final,subset=inTrain,mtry=4,importance=TRUE)
rf.Close
summary(rf.Close)
prediction = predict(rf.Close,newdata=final[-inTrain,])
Close.test=final[-inTrain,"change"]
importance(rf.Close)
varImpPlot(rf.Close)
importance(rf.Close)
#error from random forest based on Change (High-Low)
er_rf<-rmse(Close.test,prediction)
er_rf
